{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MACS 40200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General characterization of a model and data generating process\n",
    "Each of the model estimation approaches that we will discuss in this section on Maximum Likelihood estimation (MLE) and in subsequent sections on generalized method of moments (GMM) and simulated method of moments (SMM) involves choosing values of the parameters of a model to make the model match some number of properties of the data. Define a model or a data generating process (DGP) as:\n",
    "\n",
    "$$ F(x_t, z_t|\\theta) = 0 $$\n",
    "\n",
    "In reality, a model could also include inequalities representing constraints. But this is sufficient for our discussion. The goal of maximum likelihood estimation (MLE) is to choose the parameter vector of the model $\\theta$ to maximize the likelihood of seeing the data produced by the model $(x_t, z_t)$.\n",
    "\n",
    "An example of an economic model that follows the more general definition of $F(x_t, z_t|\\theta) = 0$ is Brock and Mirman (1972). This model has multiple nonlinear dynamic equations, 7 parameters, 1 exogenous time series of variables, and about 5 endogenous time series of variables. A maximum likelihood model with\n",
    "\n",
    "Another example of a model is a statistical distribution [e.g., the normal distribution $N(\\mu, \\sigma)$].\n",
    "\n",
    "$$ Pr(x|\\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "The probability of drawing value $x_i$ from the distribution $f(x|\\theta)$ is $f(x_i|\\theta)$. The probability of drawing the following vector of two observations $(x_1,x_2)$ from the distribution $f(x|\\theta)$ is $f(x_1|\\theta)\\times f(x_2|\\theta)$. We define the likelihood function of $N$ draws $(x_1,x_2,...x_N)$ from a model or distribution $f(x|\\theta)$ as $\\mathcal{L}$.\n",
    "\n",
    "$$ \\mathcal{L}(x_1,x_2,...x_N|\\theta) \\equiv \\prod_{i=1}^N f(x_i|\\theta) $$\n",
    "\n",
    "Because it can be numerically difficult to maximize a product of percentages (one small value can make dominate the entire product), it is almost always easier to use the log likelihood function $\\ln(\\mathcal{L})$.\n",
    "\n",
    "$$ \\ln\\Bigl(\\mathcal{L}(x_1,x_2,...x_N|\\theta)\\Bigr) \\equiv \\sum_{i=1}^N \\ln\\Bigl(f(x_i|\\theta)\\Bigr) $$\n",
    "\n",
    "The maximum likelihood estimate $\\hat{\\theta}_{MLE}$ is the following:\n",
    "\n",
    "$$ \\hat{\\theta}_{MLE} = \\theta:\\quad \\max_\\theta \\: \\ln\\mathcal{L} = \\sum_{i=1}^N\\ln\\Bigl(f(x_i|\\theta)\\Bigr) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparisons of distributions and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some data from the total points earned by all the students in two sections of my intermediate macroeconomics class for undergraduates at my previous University in a certain year (two semesters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "\n",
    "pts = np.loadtxt('Econ381totpts.txt')\n",
    "pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a histogram of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# This next command is specifically for Jupyter Notebook\n",
    "%matplotlib notebook\n",
    "count, bins, ignored = plt.hist(pts, 30, normed=True)\n",
    "plt.title('Econ 381 scores: 2011-2012', fontsize=20)\n",
    "plt.xlabel('Total points')\n",
    "plt.ylabel('Percent of scores')\n",
    "plt.xlim([0, 550])  # This gives the xmin and xmax to be plotted\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a distribution around these data that we think fits it well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define function that generates values of a normal pdf\n",
    "def norm_pdf(xvals, mu, sigma, cutoff):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    Generate pdf values from the normal pdf with mean mu and standard\n",
    "    deviation sigma. If the cutoff is given, then the PDF values are\n",
    "    inflated upward to reflect the zero probability on values above the\n",
    "    cutoff. If there is no cutoff given, this function does the same\n",
    "    thing as sp.stats.norm.pdf(x, loc=mu, scale=sigma).\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    xvals  = (N,) vector, values of the normally distributed random\n",
    "             variable\n",
    "    mu     = scalar, mean of the normally distributed random variable\n",
    "    sigma  = scalar > 0, standard deviation of the normally distributed\n",
    "             random variable\n",
    "    cutoff = scalar or string, ='None' if no cutoff is given, otherwise\n",
    "             is scalar upper bound value of distribution. Values above\n",
    "             this value have zero probability\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: None\n",
    "    \n",
    "    OBJECTS CREATED WITHIN FUNCTION:\n",
    "    prob_notcut = scalar \n",
    "    pdf_vals = (N,) vector, normal PDF values for mu and sigma\n",
    "               corresponding to xvals data\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: pdf_vals\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    if cutoff == 'None':\n",
    "        prob_notcut = 1.0\n",
    "    else:\n",
    "        prob_notcut = sts.norm.cdf(cutoff, loc=mu, scale=sigma)\n",
    "            \n",
    "    pdf_vals    = ((1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "                    np.exp( - (xvals - mu)**2 / (2 * sigma**2))) /\n",
    "                    prob_notcut)\n",
    "    \n",
    "    return pdf_vals\n",
    "\n",
    "dist_pts = np.linspace(0, 450, 500)\n",
    "mu_1 = 300\n",
    "sig_1 = 30\n",
    "plt.plot(dist_pts, norm_pdf(dist_pts, mu_1, sig_1, 450),\n",
    "         linewidth=2, color='r', label='1: $\\mu$=300,$\\sigma$=30')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "mu_2 = 400\n",
    "sig_2 = 60\n",
    "plt.plot(dist_pts, norm_pdf(dist_pts, mu_2, sig_2, 450),\n",
    "         linewidth=2, color='g', label='2: $\\mu$=400,$\\sigma$=60')\n",
    "plt.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which distribution will have the biggest log likelihood function? Why?\n",
    "\n",
    "Let's compute the log likelihood function for this data for both of these distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define log likelihood function for the normal distribution\n",
    "def log_lik_norm(xvals, mu, sigma, cutoff):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    Compute the log likelihood function for data xvals given normal\n",
    "    distribution parameters mu and sigma.\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    xvals  = (N,) vector, values of the normally distributed random\n",
    "             variable\n",
    "    mu     = scalar, mean of the normally distributed random variable\n",
    "    sigma  = scalar > 0, standard deviation of the normally distributed\n",
    "             random variable\n",
    "    cutoff = scalar or string, ='None' if no cutoff is given, otherwise\n",
    "             is scalar upper bound value of distribution. Values above\n",
    "             this value have zero probability\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION:\n",
    "        norm_pdf()\n",
    "    \n",
    "    OBJECTS CREATED WITHIN FUNCTION:\n",
    "    pdf_vals    = (N,) vector, normal PDF values for mu and sigma\n",
    "                  corresponding to xvals data\n",
    "    ln_pdf_vals = (N,) vector, natural logarithm of normal PDF values\n",
    "                  for mu and sigma corresponding to xvals data\n",
    "    log_lik_val = scalar, value of the log likelihood function\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: log_lik_val\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    pdf_vals = norm_pdf(xvals, mu, sigma, cutoff)\n",
    "    ln_pdf_vals = np.log(pdf_vals)\n",
    "    log_lik_val = ln_pdf_vals.sum()\n",
    "    \n",
    "    return log_lik_val\n",
    "\n",
    "print('Log-likelihood 1: ', log_lik_norm(pts, mu_1, sig_1, 450))\n",
    "print('Log-likelihood 2: ', log_lik_norm(pts, mu_2, sig_2, 450))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the log likelihood value negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we estimate $\\mu$ and $\\sigma$ by maximum likelihood? What values of $\\mu$ and $\\sigma$ will maximize the likelihood function?\n",
    "$$(\\hat{\\mu},\\hat{\\sigma})_{MLE} = (\\mu, \\sigma):\\quad argmax_{\\mu,\\sigma}\\:\\mathcal{L}=\\sum_{i=1}^N\\ln\\Bigl(f(x_i|\\mu,\\sigma)\\Bigr)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to set up a maximization (minimization) problem in Python\n",
    "A minimizer is a function that chooses a single value or a vector of values to minimize the result of a scalar-valued function of that vector. Any maximization problem can be restated as a minimization problem. Because minimization problems are more numerically stable and well defined, most numerical optimizers are stated as minimizers. The [scipy.optimize](https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.html) library has many types of root-finders and minimizers. For our maximum likelihood estimation problems, we will use the [scipy.optimize.minimize()](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. The criterion function\n",
    "The first step is to write a function that takes two inputs and returns a scalar value.\n",
    "1. The first input is either a scalar or a vector of values (the object `params` in the function `crit()` below). This object is the value or values being chosen to minimize the criterion function.\n",
    "2. The second object is Python's variable length input objects `*args`, which is a tuple of variable length positional arguments. As you will see in the `minimize()` function, all the arguments must be passed into the criterion function in one tuple.\n",
    "3. Lastly, you must make sure that the scalar criterion value that the function returns is the value of the problem stated as a minimization problem and not a maximization problem. In this case of maximum likelihood estimation, you want the negative of the log likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crit(params, *args):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    This function computes the negative of the log likelihood function\n",
    "    given parameters and data. This is the minimization problem version\n",
    "    of the maximum likelihood optimization problem\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    params = (2,) vector, ([mu, sigma])\n",
    "    mu     = scalar, mean of the normally distributed random variable\n",
    "    sigma  = scalar > 0, standard deviation of the normally distributed\n",
    "             random variable\n",
    "    args   = length 2 tuple, (xvals, cutoff)\n",
    "    xvals  = (N,) vector, values of the normally distributed random\n",
    "             variable\n",
    "    cutoff = scalar or string, ='None' if no cutoff is given, otherwise\n",
    "             is scalar upper bound value of distribution. Values above\n",
    "             this value have zero probability\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION:\n",
    "        log_lik_norm()\n",
    "    \n",
    "    OBJECTS CREATED WITHIN FUNCTION:\n",
    "    log_lik_val = scalar, value of the log likelihood function\n",
    "    neg_log_lik_val = scalar, negative of log_lik_val\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: neg_log_lik_val\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    mu, sigma = params\n",
    "    xvals, cutoff = args\n",
    "    log_lik_val = log_lik_norm(xvals, mu, sigma, cutoff)\n",
    "    neg_log_lik_val = -log_lik_val\n",
    "    \n",
    "    return neg_log_lik_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. The minimize() function\n",
    "The `minimize()` function is shorthand for `scipy.optimize.minimize()`. This function returns a dictionary of objects including the solution to the optimization problem and whether the problem actually solved. The `minimize` function has three mandatory arguments, plus a lot of options. You can experiment with the options on the [`minimize()` documentation page](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize).\n",
    "1. The first argument of the minimize function is the criterion function (`crit()` in this example) from which the `minimize()` function will test values of the parameters in searching for the minimum value.\n",
    "2. The second argument is an initial guess for the values of the parameters that minimize the criterion function `crit()`.\n",
    "3. The third argument is the tuple of all the objects needed to solve the criterion function in `crit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "mu_init = 400  # mu_2\n",
    "sig_init = 70  # sig_2\n",
    "params_init = np.array([mu_init, sig_init])\n",
    "mle_args = (pts, 450.0)\n",
    "results = opt.minimize(crit, params_init, args=(mle_args))\n",
    "mu_MLE, sig_MLE = results.x\n",
    "print('mu_MLE=', mu_MLE, ' sig_MLE=', sig_MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the histogram of the data\n",
    "count, bins, ignored = plt.hist(pts, 30, normed=True)\n",
    "plt.title('Econ 381 scores: 2011-2012', fontsize=20)\n",
    "plt.xlabel('Total points')\n",
    "plt.ylabel('Percent of scores')\n",
    "plt.xlim([0, 550])  # This gives the xmin and xmax to be plotted\"\n",
    "\n",
    "# Plot the two test distributions from before\n",
    "plt.plot(dist_pts, norm_pdf(dist_pts, mu_1, sig_1, 450),\n",
    "         linewidth=2, color='r', label='1: $\\mu$=300,$\\sigma$=30')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.plot(dist_pts, norm_pdf(dist_pts, mu_2, sig_2, 450),\n",
    "         linewidth=2, color='g', label='2: $\\mu$=400,$\\sigma$=70')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot the MLE estimated distribution\n",
    "plt.plot(dist_pts, norm_pdf(dist_pts, mu_MLE, sig_MLE, 450),\n",
    "         linewidth=2, color='k', label='3: $\\mu$=558,$\\sigma$=176')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Log-likelihood 1: ', log_lik_norm(pts, mu_1, sig_1, 450))\n",
    "print('Log-likelihood 2: ', log_lik_norm(pts, mu_2, sig_2, 450))\n",
    "print('MLE log-likelihood 3: ', log_lik_norm(pts, mu_MLE, sig_MLE, 450))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this likelihood function is well behaved by looking at a grid over possible values of $\\mu$ and $\\sigma$ for the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "cmap1 = matplotlib.cm.get_cmap('summer')\n",
    "\n",
    "mu_vals = np.linspace(540, 590, 50)\n",
    "sig_vals = np.linspace(170, 180, 50)\n",
    "lnlik_vals = np.zeros((50, 50))\n",
    "for mu_ind in range(50):\n",
    "    for sig_ind in range(50):\n",
    "        lnlik_vals[mu_ind, sig_ind] = log_lik_norm(pts, mu_vals[mu_ind],\n",
    "                                                   sig_vals[sig_ind], 450)\n",
    "\n",
    "mu_mesh, sig_mesh = np.meshgrid(mu_vals, sig_vals)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(sig_mesh, mu_mesh, lnlik_vals, rstride=8,\n",
    "                cstride=1, cmap=cmap1)\n",
    "ax.set_title('Log likelihood for values of mu and sigma')\n",
    "ax.set_xlabel(r'$\\sigma$')\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_zlabel(r'log likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The variance-covariance matrix of ML estimates\n",
    "Davidson and MacKinnon (2004, Sec. 10.4) have a great discussion four different estimators for the variance-covariance matrix of the maximum likelihood estimates. That is, we want to know what is the variance or uncertainty of our estimates for $\\mu$ and $\\sigma$, and how are those two estimates correlated. The four most common estimators for the VCV matrix of a maximum likelihood estimate are:\n",
    "1. Empirical Hessian estimator (H)\n",
    "2. Information matrix estimator (I)\n",
    "3. Outer-product-of-the-gradient estimator (OPG)\n",
    "4. Sandwich estimator (S)\n",
    "\n",
    "All of these estimators of the VCV matrix intuitively measure how flat the likelihood function is at the estimated parameter values in the dimension of each estimated parameter. The Hessian is a matrix of second derivatives of the log likelihood function with respect to the parameters being chosen. The Hessian matrix therefore captures information about how the slope of the log likelihood function is changing in each direction.The empirical Hessian estimator is the most commonly used. One really nice property of Python's `minimize()` function is that one of the result objects is the inverse Hessian.\n",
    "\n",
    "$$ \\hat{VAR}_H(\\hat{\\theta}) =-H^{-1}(\\hat{\\theta}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results\n",
    "vcv_mle = results.hess_inv\n",
    "stderr_mu_mle = np.sqrt(vcv_mle[0,0])\n",
    "stderr_sig_mle = np.sqrt(vcv_mle[1,1])\n",
    "print('VCV(MLE) = ', vcv_mle)\n",
    "print('Standard error for mu estimate = ', stderr_mu_mle)\n",
    "print('Standard error for sigma estimate = ', stderr_sig_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hypothesis testing\n",
    "Can we reject the hypothesis that $\\mu=400$ and $\\sigma=70$ with 95% confidence? How do you answer that question? What does the figure tell us about this answer? In this section, we will discuss four ways to perform hypothesis testing.\n",
    "1. Two standard errors (back of the envelope, approximation)\n",
    "2. Likelihood ratio test\n",
    "3. Wald test\n",
    "4. Lagrange multiplier test\n",
    "\n",
    "Davidson and MacKinnon (2004, Sec. 10.5) have a more detailed discussion of methods 2, 3, and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Back of the envelope, two standard errors (assuming normality)\n",
    "A really quick approach to hypothesis testing is to see if your hypothesized values are within two standard errors of the estimated values. This approach is not completely correct because estimates in the log likelihood function are not symmetrically distributed. But it is at least a first approximation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb_mu_95pctci = mu_MLE - 2 * stderr_mu_mle\n",
    "print('mu_2=', mu_2, ', lower bound 95% conf. int.=', lb_mu_95pctci)\n",
    "\n",
    "lb_sig_95pctci = sig_MLE - 2 * stderr_sig_mle\n",
    "print('sig_2=', sig_2, ', lower bound 95% conf. int.=', lb_sig_95pctci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Likelihood ratio test\n",
    "The likelihood ratio test is the simplest and, therefore, the most common of the three more precise methods (2, 3, and 4). Let your maximum likelihood estimation have $p$ parameters (the vector $\\theta$ has $p$ elements), let $\\hat{\\theta}_{MLE}$ be the maximum likelihood estimate, and let $\\tilde{\\theta}$ be your hypothesized values of the parameters. The likelihood ratio test statistic is the following.\n",
    "\n",
    "$$ LR(\\tilde{\\theta}|\\hat{\\theta}_{MLE}) = 2\\Bigl(\\ln\\ell(\\hat{\\theta}_{MLE}) - \\ln\\ell(\\tilde{\\theta})\\Bigr) \\sim \\chi^2(p) $$\n",
    "\n",
    "Note that this is a joint test of the likelihood of $H_0: \\mu_0, \\sigma_0$. The value of the $\\chi^2(p)$ tells you the probability that the data could have been taken from the hypothesized distribution or model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_lik_h0 = log_lik_norm(pts, mu_2, sig_2, 450)\n",
    "# log_lik_h0 = log_lik_norm(pts, 555, 176, 450)\n",
    "log_lik_mle = log_lik_norm(pts, mu_MLE, sig_MLE, 450)\n",
    "LR_val = 2 * (log_lik_mle - log_lik_h0)\n",
    "chi2_ho = sts.chi2.pdf(LR_val, 2)\n",
    "print('chi squared of H0 with 2 degrees of freedom = ', chi2_ho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generalized beta family of distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. References\n",
    "* Brock, William A. and Leonard J. Mirman, \"Optimal Economic Growth and Uncertainty: The Discounted Case,\" *Journal of Economic Theory*, 4:3, pp. 479-513 (June 1972).\n",
    "* Davidson, Russell and James G. MacKinnon, *Econometric Theory and Methods*, Oxford University Press (2004)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
